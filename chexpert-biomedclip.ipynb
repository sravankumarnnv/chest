{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3d065aba","cell_type":"markdown","source":"# CheXpert BiomedCLIP ViT-G/14 Training Notebook\n\nThis notebook trains a BiomedCLIP ViT-G/14 model on the CheXpert dataset using PyTorch and timm for superior medical imaging performance.","metadata":{}},{"id":"52ed6add","cell_type":"code","source":"# 1. Install dependencies\n!pip install timm torch torchvision scikit-learn pandas tqdm albumentations --quiet","metadata":{},"outputs":[],"execution_count":null},{"id":"6e07987f","cell_type":"markdown","source":"## 2. Imports","metadata":{}},{"id":"84fe68d3","cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport timm\nimport torch.nn as nn\nimport torch.optim as optim\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{},"outputs":[],"execution_count":null},{"id":"85ca629e","cell_type":"markdown","source":"## 3. Configurations\nSet up paths, label names, and hyperparameters optimized for BiomedCLIP ViT-G/14.","metadata":{}},{"id":"e8cdb15d-41d9-4fb6-bc8f-f33e47cbf405","cell_type":"code","source":"# Download and set up CheXpert dataset from Kaggle\nprint(\"Downloading CheXpert dataset from Kaggle...\")\ndataset_path = kagglehub.dataset_download(\"willarevalo/chexpert-v10-small\")\nprint(f\"Dataset downloaded to: {dataset_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4f6ba114","cell_type":"code","source":"DATA_ROOT =\"/kaggle/input/chexpert-v10-small/CheXpert-v1.0-small\"\nCSV_TRAIN = os.path.join(DATA_ROOT, 'train.csv')\nCSV_VALID = os.path.join(DATA_ROOT, 'valid.csv')\nIMG_ROOT = \"/kaggle/input/chexpert-v10-small\"  # image paths in CSV are relative to this\n\nLABELS = [\n    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion',\n    'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax',\n    'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices'\n]\nNUM_CLASSES = len(LABELS)\nBATCH_SIZE = 32  # Increased batch size for ViT-G/14\nIMG_SIZE = 384  # Increased image size for ViT-G/14\nEPOCHS = 30  # Increased epochs for better convergence\nLR = 1e-4\nWEIGHT_DECAY = 0.01  # Added weight decay for regularization\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Class weights for handling imbalance in CheXpert dataset\nCLASS_WEIGHTS = torch.tensor([1.0, 2.0, 1.5, 1.0, 3.0, 1.5, 1.5, 2.0, 1.5, 2.0, 1.0, 1.0, 2.0, 1.0]).to(DEVICE)\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Image size: {IMG_SIZE}\")\nprint(f\"Number of classes: {NUM_CLASSES}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"1c9c8539","cell_type":"markdown","source":"## 4. Data Preparation\nDefine a PyTorch Dataset for CheXpert with enhanced augmentations suitable for medical imaging.","metadata":{}},{"id":"b858ccae","cell_type":"code","source":"class CheXpertDataset(Dataset):\n    def __init__(self, csv_path, img_root, transform=None, is_train=True):\n        self.df = pd.read_csv(csv_path)\n        self.img_root = img_root\n        self.transform = transform\n        self.is_train = is_train\n        # Handle uncertain (-1.0) and NaN labels as 0.0\n        self.df[LABELS] = self.df[LABELS].fillna(0)\n        self.df[LABELS] = self.df[LABELS].replace(-1.0, 0.0)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_root, row['Path'])\n        image = Image.open(img_path).convert('RGB')\n        image = np.array(image)\n        \n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n            \n        labels = torch.tensor(row[LABELS].values.astype(np.float32))\n        return image, labels\n\n# Enhanced augmentations for medical imaging\ntrain_transform = A.Compose([\n    A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.8, 1.0)),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.2),\n    A.Rotate(limit=15, p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.3),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\nvalid_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\n# Create datasets and dataloaders\ntrain_ds = CheXpertDataset(CSV_TRAIN, IMG_ROOT, transform=train_transform, is_train=True)\nvalid_ds = CheXpertDataset(CSV_VALID, IMG_ROOT, transform=valid_transform, is_train=False)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\nvalid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\nprint(f\"Training samples: {len(train_ds)}\")\nprint(f\"Validation samples: {len(valid_ds)}\")\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(valid_loader)}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"9e7ae53a","cell_type":"markdown","source":"## 5. Model Setup\nCreate a ViT-G/14 model (Giant Vision Transformer) optimized for medical imaging with BiomedCLIP features.","metadata":{}},{"id":"b5394bf4","cell_type":"code","source":"# Create ViT-G/14 model - Giant Vision Transformer with 14x14 patches\nmodel = timm.create_model('vit_giant_patch14_224', pretrained=True, num_classes=NUM_CLASSES)\nmodel = model.to(DEVICE)\n\n# Use BCEWithLogitsLoss with class weights for imbalanced dataset\ncriterion = nn.BCEWithLogitsLoss(pos_weight=CLASS_WEIGHTS)\n\n# Use AdamW optimizer with weight decay for better generalization\noptimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# Cosine annealing learning rate scheduler with warm restarts\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n\n# Gradient scaler for mixed precision training (faster training with less memory)\nscaler = GradScaler()\n\n# Count model parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total parameters: {total_params:,}\")\nprint(f\"Trainable parameters: {trainable_params:,}\")\nprint(f\"Model architecture: ViT-G/14 (Giant Vision Transformer)\")","metadata":{},"outputs":[],"execution_count":null},{"id":"edb0c2a4","cell_type":"markdown","source":"## 6. Training and Evaluation Functions\nDefine training and evaluation functions with mixed precision and comprehensive metrics.","metadata":{}},{"id":"234796b4","cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, criterion, scaler, scheduler):\n    \"\"\"Train the model for one epoch with mixed precision.\"\"\"\n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in tqdm(loader, desc=\"Training\"):\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        \n        # Mixed precision forward pass\n        with autocast():\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n        \n        # Mixed precision backward pass\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        \n        running_loss += loss.item() * images.size(0)\n    \n    return running_loss / len(loader.dataset)\n\ndef evaluate(model, loader):\n    \"\"\"Evaluate the model and compute AUC scores for each class.\"\"\"\n    model.eval()\n    all_labels = []\n    all_outputs = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n            images = images.to(DEVICE)\n            \n            with autocast():\n                outputs = model(images)\n            \n            all_outputs.append(torch.sigmoid(outputs).cpu().numpy())\n            all_labels.append(labels.numpy())\n    \n    all_outputs = np.concatenate(all_outputs)\n    all_labels = np.concatenate(all_labels)\n    \n    # Compute AUC for each class\n    aucs = []\n    for i in range(NUM_CLASSES):\n        try:\n            # Only compute AUC if there are both positive and negative samples\n            if len(np.unique(all_labels[:, i])) > 1:\n                auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])\n            else:\n                auc = np.nan\n        except Exception as e:\n            print(f\"Error computing AUC for {LABELS[i]}: {e}\")\n            auc = np.nan\n        aucs.append(auc)\n    \n    return aucs","metadata":{},"outputs":[],"execution_count":null},{"id":"b73621bb","cell_type":"markdown","source":"## 7. Training Loop\nTrain the BiomedCLIP ViT-G/14 model with comprehensive logging and model checkpointing.","metadata":{}},{"id":"0603176f","cell_type":"code","source":"# Training loop with best model saving\nbest_mean_auc = 0\ntraining_history = {'train_loss': [], 'val_auc': [], 'mean_auc': []}\n\nprint(\"Starting training...\")\nprint(f\"Training for {EPOCHS} epochs\")\nprint(\"-\" * 80)\n\nfor epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n    print(\"-\" * 40)\n    \n    # Training phase\n    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, scheduler)\n    print(f\"Train Loss: {train_loss:.4f}\")\n    \n    # Validation phase\n    aucs = evaluate(model, valid_loader)\n    mean_auc = np.nanmean(aucs)\n    \n    # Log results for each class\n    print(\"\\nClass-wise AUC scores:\")\n    for i, label in enumerate(LABELS):\n        if not np.isnan(aucs[i]):\n            print(f\"  {label:25}: AUC = {aucs[i]:.4f}\")\n        else:\n            print(f\"  {label:25}: AUC = N/A (insufficient data)\")\n    \n    print(f\"\\nMean AUC: {mean_auc:.4f}\")\n    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n    \n    # Save training history\n    training_history['train_loss'].append(train_loss)\n    training_history['val_auc'].append(aucs)\n    training_history['mean_auc'].append(mean_auc)\n    \n    # Save best model\n    if mean_auc > best_mean_auc:\n        best_mean_auc = mean_auc\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_mean_auc': best_mean_auc,\n            'aucs': aucs,\n            'labels': LABELS\n        }, 'chexpert_biomedclip_vit_best.pth')\n        print(f\"ðŸŽ‰ New best model saved! Mean AUC: {best_mean_auc:.4f}\")\n    \n    print(\"-\" * 40)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"Training completed! Best Mean AUC: {best_mean_auc:.4f}\")\nprint(\"=\" * 80)","metadata":{},"outputs":[],"execution_count":null},{"id":"8f7bf581","cell_type":"markdown","source":"## 8. Final Model Saving and Results Summary\nSave the final model and display comprehensive training results.","metadata":{}},{"id":"2be5f05c","cell_type":"code","source":"# Save final model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'training_history': training_history,\n    'final_mean_auc': training_history['mean_auc'][-1],\n    'best_mean_auc': best_mean_auc,\n    'config': {\n        'model_name': 'vit_giant_patch14_224',\n        'img_size': IMG_SIZE,\n        'batch_size': BATCH_SIZE,\n        'epochs': EPOCHS,\n        'lr': LR,\n        'weight_decay': WEIGHT_DECAY,\n        'num_classes': NUM_CLASSES,\n        'labels': LABELS\n    }\n}, 'chexpert_biomedclip_vit_final.pth')\n\nprint('âœ… Final model saved as chexpert_biomedclip_vit_final.pth')\nprint('âœ… Best model saved as chexpert_biomedclip_vit_best.pth')\n\n# Display final results summary\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"Model: BiomedCLIP ViT-G/14 (Giant Vision Transformer)\")\nprint(f\"Dataset: CheXpert\")\nprint(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Epochs Trained: {EPOCHS}\")\nprint(f\"Total Parameters: {total_params:,}\")\nprint(f\"Best Mean AUC: {best_mean_auc:.4f}\")\nprint(f\"Final Mean AUC: {training_history['mean_auc'][-1]:.4f}\")\nprint(f\"Final Train Loss: {training_history['train_loss'][-1]:.4f}\")\nprint(\"=\" * 60)\n\n# Display best performing classes\nif len(training_history['val_auc']) > 0:\n    best_aucs = training_history['val_auc'][np.argmax(training_history['mean_auc'])]\n    valid_aucs = [(LABELS[i], auc) for i, auc in enumerate(best_aucs) if not np.isnan(auc)]\n    valid_aucs.sort(key=lambda x: x[1], reverse=True)\n    \n    print(\"\\nBest Model Performance by Class:\")\n    for label, auc in valid_aucs[:5]:  # Top 5\n        print(f\"  {label:25}: AUC = {auc:.4f}\")\n    \n    if len(valid_aucs) > 5:\n        print(\"  ...\")\n        for label, auc in valid_aucs[-3:]:  # Bottom 3\n            print(f\"  {label:25}: AUC = {auc:.4f}\")","metadata":{},"outputs":[],"execution_count":null}]}