{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d065aba",
   "metadata": {},
   "source": [
    "# CheXpert BiomedCLIP ViT-G/14 Training Notebook\n",
    "\n",
    "This notebook trains a BiomedCLIP ViT-G/14 model on the CheXpert dataset using PyTorch and timm for superior medical imaging performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install dependencies for BiomedCLIP training\n",
    "!pip install timm torch torchvision scikit-learn pandas tqdm albumentations --quiet\n",
    "!pip install open_clip_torch transformers datasets --quiet\n",
    "!pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07987f",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# BiomedCLIP imports\n",
    "try:\n",
    "    import open_clip\n",
    "    OPENCLIP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENCLIP_AVAILABLE = False\n",
    "    print(\"âš ï¸ open_clip not available\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModel, AutoProcessor, CLIPModel, CLIPProcessor\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"âš ï¸ transformers not available\")\n",
    "\n",
    "print(f\"OpenCLIP available: {OPENCLIP_AVAILABLE}\")\n",
    "print(f\"Transformers available: {TRANSFORMERS_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca629e",
   "metadata": {},
   "source": [
    "## 3. Configurations\n",
    "Set up paths, label names, and hyperparameters optimized for BiomedCLIP ViT-G/14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cdb15d-41d9-4fb6-bc8f-f33e47cbf405",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download and set up CheXpert dataset from Kaggle\n",
    "print(\"Downloading CheXpert dataset from Kaggle...\")\n",
    "dataset_path = kagglehub.dataset_download(\"willarevalo/chexpert-v10-small\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ba114",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT =\"/kaggle/input/chexpert-v10-small/CheXpert-v1.0-small\"\n",
    "CSV_TRAIN = os.path.join(DATA_ROOT, 'train.csv')\n",
    "CSV_VALID = os.path.join(DATA_ROOT, 'valid.csv')\n",
    "IMG_ROOT = \"/kaggle/input/chexpert-v10-small\"  # image paths in CSV are relative to this\n",
    "\n",
    "LABELS = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion',\n",
    "    'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax',\n",
    "    'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices'\n",
    "]\n",
    "NUM_CLASSES = len(LABELS)\n",
    "\n",
    "# Optimized hyperparameters for BiomedCLIP and 95%+ accuracy\n",
    "BATCH_SIZE = 64  # Increased for better gradient estimates\n",
    "IMG_SIZE = 224  # BiomedCLIP standard size\n",
    "EPOCHS = 50  # Increased for better convergence\n",
    "LR_BACKBONE = 1e-5  # Very low LR for pre-trained backbone\n",
    "LR_HEAD = 1e-3  # Higher LR for classification head\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_EPOCHS = 5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Enhanced class weights for better balance\n",
    "CLASS_WEIGHTS = torch.tensor([0.8, 3.0, 2.0, 1.2, 4.0, 2.5, 2.5, 3.0, 2.0, 3.5, 1.5, 1.5, 3.0, 1.2]).to(DEVICE)\n",
    "\n",
    "# Training strategy flags\n",
    "FREEZE_BACKBONE = True  # Start with frozen backbone\n",
    "USE_FOCAL_LOSS = True  # Better for imbalanced data\n",
    "USE_LABEL_SMOOTHING = True  # Regularization technique\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Image size: {IMG_SIZE}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Freeze backbone: {FREEZE_BACKBONE}\")\n",
    "print(f\"Use focal loss: {USE_FOCAL_LOSS}\")\n",
    "print(f\"Use label smoothing: {USE_LABEL_SMOOTHING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c8539",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "Define a PyTorch Dataset for CheXpert with enhanced augmentations suitable for medical imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_root, transform=None, is_train=True):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Enhanced label handling for better accuracy\n",
    "        # Handle uncertain (-1.0) as 0.0 and NaN as 0.0\n",
    "        self.df[LABELS] = self.df[LABELS].fillna(0)\n",
    "        self.df[LABELS] = self.df[LABELS].replace(-1.0, 0.0)\n",
    "        \n",
    "        # Apply label smoothing if enabled\n",
    "        if USE_LABEL_SMOOTHING and is_train:\n",
    "            smoothing = 0.1\n",
    "            self.df[LABELS] = self.df[LABELS] * (1 - smoothing) + smoothing / 2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_root, row['Path'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        labels = torch.tensor(row[LABELS].values.astype(np.float32))\n",
    "        return image, labels\n",
    "\n",
    "# BiomedCLIP optimized transforms\n",
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.85, 1.0)),  # Less aggressive cropping for medical images\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.3),\n",
    "    A.Rotate(limit=10, p=0.3),  # Reduced rotation for medical accuracy\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.3),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.1),  # Medical image specific augmentation\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.2),  # Contrast enhancement\n",
    "    # BiomedCLIP normalization\n",
    "    A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "               std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    # BiomedCLIP normalization\n",
    "    A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "               std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ds = CheXpertDataset(CSV_TRAIN, IMG_ROOT, transform=train_transform, is_train=True)\n",
    "valid_ds = CheXpertDataset(CSV_VALID, IMG_ROOT, transform=valid_transform, is_train=False)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(valid_ds)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ae53a",
   "metadata": {},
   "source": [
    "## 5. BiomedCLIP Model Setup Options\n",
    "Choose one of the following methods to load BiomedCLIP. Run only ONE of the following cells based on availability and preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5394bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: BiomedCLIP via OpenCLIP (Recommended)\n",
    "# Run this cell if open_clip is available\n",
    "\n",
    "if OPENCLIP_AVAILABLE:\n",
    "    try:\n",
    "        print(\"Loading BiomedCLIP via OpenCLIP...\")\n",
    "        \n",
    "        # Load BiomedCLIP model\n",
    "        clip_model, _, _ = open_clip.create_model_and_transforms(\n",
    "            'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n",
    "        )\n",
    "        \n",
    "        class BiomedCLIPClassifier(nn.Module):\n",
    "            def __init__(self, clip_model, num_classes, freeze_backbone=True):\n",
    "                super().__init__()\n",
    "                self.clip_model = clip_model\n",
    "                self.freeze_backbone = freeze_backbone\n",
    "                \n",
    "                # Freeze backbone if specified\n",
    "                if freeze_backbone:\n",
    "                    for param in self.clip_model.parameters():\n",
    "                        param.requires_grad = False\n",
    "                    print(\"ðŸ”’ Backbone frozen for initial training\")\n",
    "                else:\n",
    "                    print(\"ðŸ”“ Backbone unfrozen for fine-tuning\")\n",
    "                \n",
    "                # Enhanced classification head for better performance\n",
    "                feature_dim = clip_model.visual.output_dim\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(feature_dim, 1024),\n",
    "                    nn.BatchNorm1d(1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.BatchNorm1d(512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(512, num_classes)\n",
    "                )\n",
    "                \n",
    "            def forward(self, images):\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    image_features = self.clip_model.encode_image(images)\n",
    "                    if self.freeze_backbone:\n",
    "                        image_features = image_features.detach()\n",
    "                    return self.classifier(image_features)\n",
    "            \n",
    "            def unfreeze_backbone(self):\n",
    "                \"\"\"Unfreeze backbone for fine-tuning\"\"\"\n",
    "                for param in self.clip_model.parameters():\n",
    "                    param.requires_grad = True\n",
    "                self.freeze_backbone = False\n",
    "                print(\"ðŸ”“ Backbone unfrozen for fine-tuning\")\n",
    "        \n",
    "        model = BiomedCLIPClassifier(clip_model, NUM_CLASSES, freeze_backbone=FREEZE_BACKBONE)\n",
    "        model = model.to(DEVICE)\n",
    "        \n",
    "        MODEL_TYPE = \"BiomedCLIP-OpenCLIP\"\n",
    "        print(f\"âœ… Successfully loaded {MODEL_TYPE}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load BiomedCLIP via OpenCLIP: {e}\")\n",
    "        model = None\n",
    "else:\n",
    "    print(\"âŒ OpenCLIP not available. Try Option 2 or 3.\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d425c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: BiomedCLIP via Transformers (Alternative)\n",
    "# Run this cell if transformers is available and Option 1 failed\n",
    "\n",
    "if TRANSFORMERS_AVAILABLE and model is None:\n",
    "    try:\n",
    "        print(\"Loading BiomedCLIP via Transformers...\")\n",
    "        \n",
    "        # Load BiomedCLIP using transformers\n",
    "        model_name = \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n",
    "        clip_model = CLIPModel.from_pretrained(model_name)\n",
    "        processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        class BiomedCLIPTransformersClassifier(nn.Module):\n",
    "            def __init__(self, clip_model, num_classes, freeze_backbone=True):\n",
    "                super().__init__()\n",
    "                self.clip_model = clip_model\n",
    "                self.freeze_backbone = freeze_backbone\n",
    "                \n",
    "                # Freeze backbone if specified\n",
    "                if freeze_backbone:\n",
    "                    for param in self.clip_model.parameters():\n",
    "                        param.requires_grad = False\n",
    "                    print(\"ðŸ”’ Backbone frozen for initial training\")\n",
    "                \n",
    "                # Classification head\n",
    "                feature_dim = clip_model.config.projection_dim\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(feature_dim, 1024),\n",
    "                    nn.BatchNorm1d(1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.BatchNorm1d(512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(512, num_classes)\n",
    "                )\n",
    "                \n",
    "            def forward(self, images):\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    image_features = self.clip_model.get_image_features(images)\n",
    "                    if self.freeze_backbone:\n",
    "                        image_features = image_features.detach()\n",
    "                    return self.classifier(image_features)\n",
    "            \n",
    "            def unfreeze_backbone(self):\n",
    "                for param in self.clip_model.parameters():\n",
    "                    param.requires_grad = True\n",
    "                self.freeze_backbone = False\n",
    "                print(\"ðŸ”“ Backbone unfrozen for fine-tuning\")\n",
    "        \n",
    "        model = BiomedCLIPTransformersClassifier(clip_model, NUM_CLASSES, freeze_backbone=FREEZE_BACKBONE)\n",
    "        model = model.to(DEVICE)\n",
    "        \n",
    "        MODEL_TYPE = \"BiomedCLIP-Transformers\"\n",
    "        print(f\"âœ… Successfully loaded {MODEL_TYPE}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load BiomedCLIP via Transformers: {e}\")\n",
    "        model = None\n",
    "else:\n",
    "    if model is not None:\n",
    "        print(\"âœ… Model already loaded, skipping Option 2\")\n",
    "    else:\n",
    "        print(\"âŒ Transformers not available. Try Option 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 3: Enhanced ViT Fallback (if BiomedCLIP unavailable)\n",
    "# Run this cell if both Option 1 and 2 failed\n",
    "\n",
    "if model is None:\n",
    "    print(\"Loading enhanced ViT as fallback...\")\n",
    "    \n",
    "    # Use the largest available ViT model\n",
    "    try:\n",
    "        # Try ViT-Large first\n",
    "        base_model = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=0)  # No head\n",
    "        feature_dim = base_model.num_features\n",
    "        model_name = 'vit_large_patch16_224'\n",
    "    except:\n",
    "        # Fallback to ViT-Base\n",
    "        base_model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "        feature_dim = base_model.num_features\n",
    "        model_name = 'vit_base_patch16_224'\n",
    "    \n",
    "    class EnhancedViTClassifier(nn.Module):\n",
    "        def __init__(self, base_model, feature_dim, num_classes, freeze_backbone=True):\n",
    "            super().__init__()\n",
    "            self.backbone = base_model\n",
    "            self.freeze_backbone = freeze_backbone\n",
    "            \n",
    "            if freeze_backbone:\n",
    "                for param in self.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "                print(\"ðŸ”’ Backbone frozen for initial training\")\n",
    "            \n",
    "            # Enhanced classification head\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(feature_dim, 1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            features = self.backbone(x)\n",
    "            if self.freeze_backbone:\n",
    "                features = features.detach()\n",
    "            return self.classifier(features)\n",
    "        \n",
    "        def unfreeze_backbone(self):\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "            self.freeze_backbone = False\n",
    "            print(\"ðŸ”“ Backbone unfrozen for fine-tuning\")\n",
    "    \n",
    "    model = EnhancedViTClassifier(base_model, feature_dim, NUM_CLASSES, freeze_backbone=FREEZE_BACKBONE)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    MODEL_TYPE = f\"Enhanced-{model_name}\"\n",
    "    print(f\"âœ… Successfully loaded {MODEL_TYPE} as fallback\")\n",
    "\n",
    "# Model summary\n",
    "if model is not None:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nModel: {MODEL_TYPE}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "else:\n",
    "    raise RuntimeError(\"âŒ Failed to load any model. Check your installations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2930fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Loss Functions and Optimizer Setup\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Setup loss function\n",
    "if USE_FOCAL_LOSS:\n",
    "    criterion = FocalLoss(alpha=1, gamma=2)\n",
    "    print(\"âœ… Using Focal Loss for better class balance\")\n",
    "else:\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=CLASS_WEIGHTS)\n",
    "    print(\"âœ… Using weighted BCE Loss\")\n",
    "\n",
    "# Setup optimizer with different learning rates\n",
    "if hasattr(model, 'classifier'):\n",
    "    if FREEZE_BACKBONE:\n",
    "        # Only train classifier when backbone is frozen\n",
    "        optimizer = optim.AdamW(model.classifier.parameters(), lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
    "        print(f\"âœ… Optimizer setup for frozen backbone (LR: {LR_HEAD})\")\n",
    "    else:\n",
    "        # Different learning rates for backbone and classifier\n",
    "        if hasattr(model, 'clip_model'):\n",
    "            backbone_params = model.clip_model.parameters()\n",
    "        else:\n",
    "            backbone_params = model.backbone.parameters()\n",
    "            \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': LR_BACKBONE},\n",
    "            {'params': model.classifier.parameters(), 'lr': LR_HEAD}\n",
    "        ], weight_decay=WEIGHT_DECAY)\n",
    "        print(f\"âœ… Optimizer setup for fine-tuning (Backbone LR: {LR_BACKBONE}, Head LR: {LR_HEAD})\")\n",
    "else:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR_HEAD, weight_decay=WEIGHT_DECAY)\n",
    "    print(f\"âœ… Standard optimizer setup (LR: {LR_HEAD})\")\n",
    "\n",
    "# Enhanced learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LR_HEAD if FREEZE_BACKBONE else [LR_BACKBONE, LR_HEAD],\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "# Gradient scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"âœ… Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0c2a4",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions\n",
    "Define training and evaluation functions with mixed precision and comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234796b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, scaler, scheduler):\n",
    "    \"\"\"Train the model for one epoch with mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Mixed precision backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Evaluate the model and compute AUC scores for each class.\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "            \n",
    "            all_outputs.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Compute AUC for each class\n",
    "    aucs = []\n",
    "    for i in range(NUM_CLASSES):\n",
    "        try:\n",
    "            # Only compute AUC if there are both positive and negative samples\n",
    "            if len(np.unique(all_labels[:, i])) > 1:\n",
    "                auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])\n",
    "            else:\n",
    "                auc = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing AUC for {LABELS[i]}: {e}\")\n",
    "            auc = np.nan\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73621bb",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "Train the BiomedCLIP ViT-G/14 model with comprehensive logging and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Training Loop with Two-Stage Training Strategy\n",
    "best_mean_auc = 0\n",
    "training_history = {'train_loss': [], 'val_auc': [], 'mean_auc': []}\n",
    "\n",
    "print(\"Starting Enhanced Two-Stage Training...\")\n",
    "print(f\"Stage 1: Frozen backbone training ({EPOCHS//2} epochs)\")\n",
    "print(f\"Stage 2: Fine-tuning entire model ({EPOCHS//2} epochs)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Stage 1: Train with frozen backbone\n",
    "print(\"\\n=== STAGE 1: FROZEN BACKBONE TRAINING ===\")\n",
    "for epoch in range(EPOCHS//2):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS//2} (Stage 1)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, scheduler)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    aucs = evaluate(model, valid_loader)\n",
    "    mean_auc = np.nanmean(aucs)\n",
    "    \n",
    "    # Log results for each class\n",
    "    print(\"\\nClass-wise AUC scores:\")\n",
    "    for i, label in enumerate(LABELS):\n",
    "        if not np.isnan(aucs[i]):\n",
    "            print(f\"  {label:25}: AUC = {aucs[i]:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {label:25}: AUC = N/A (insufficient data)\")\n",
    "    \n",
    "    print(f\"\\nMean AUC: {mean_auc:.4f}\")\n",
    "    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save training history\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_auc'].append(aucs)\n",
    "    training_history['mean_auc'].append(mean_auc)\n",
    "    \n",
    "    # Save best model\n",
    "    if mean_auc > best_mean_auc:\n",
    "        best_mean_auc = mean_auc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_mean_auc': best_mean_auc,\n",
    "            'aucs': aucs,\n",
    "            'labels': LABELS,\n",
    "            'stage': 1\n",
    "        }, 'chexpert_biomedclip_vit_best.pth')\n",
    "        print(f\"ðŸŽ‰ New best model saved! Mean AUC: {best_mean_auc:.4f}\")\n",
    "\n",
    "# Stage 2: Unfreeze backbone for fine-tuning\n",
    "print(\"\\n=== STAGE 2: FINE-TUNING ENTIRE MODEL ===\")\n",
    "if hasattr(model, 'unfreeze_backbone'):\n",
    "    model.unfreeze_backbone()\n",
    "    \n",
    "    # Create new optimizer with different learning rates for backbone and head\n",
    "    if hasattr(model, 'clip_model'):\n",
    "        backbone_params = model.clip_model.parameters()\n",
    "    else:\n",
    "        backbone_params = model.backbone.parameters()\n",
    "        \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': LR_BACKBONE},\n",
    "        {'params': model.classifier.parameters(), 'lr': LR_HEAD}\n",
    "    ], weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # New scheduler for fine-tuning\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=[LR_BACKBONE, LR_HEAD],\n",
    "        epochs=EPOCHS//2,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.2,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Fine-tuning setup complete (Backbone LR: {LR_BACKBONE}, Head LR: {LR_HEAD})\")\n",
    "\n",
    "for epoch in range(EPOCHS//2, EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} (Stage 2)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, scheduler)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    aucs = evaluate(model, valid_loader)\n",
    "    mean_auc = np.nanmean(aucs)\n",
    "    \n",
    "    # Log results for each class\n",
    "    print(\"\\nClass-wise AUC scores:\")\n",
    "    for i, label in enumerate(LABELS):\n",
    "        if not np.isnan(aucs[i]):\n",
    "            print(f\"  {label:25}: AUC = {aucs[i]:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {label:25}: AUC = N/A (insufficient data)\")\n",
    "    \n",
    "    print(f\"\\nMean AUC: {mean_auc:.4f}\")\n",
    "    print(f\"Current Backbone LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Current Head LR: {optimizer.param_groups[1]['lr']:.6f}\")\n",
    "    \n",
    "    # Save training history\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_auc'].append(aucs)\n",
    "    training_history['mean_auc'].append(mean_auc)\n",
    "    \n",
    "    # Save best model\n",
    "    if mean_auc > best_mean_auc:\n",
    "        best_mean_auc = mean_auc\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_mean_auc': best_mean_auc,\n",
    "            'aucs': aucs,\n",
    "            'labels': LABELS,\n",
    "            'stage': 2\n",
    "        }, 'chexpert_biomedclip_vit_best.pth')\n",
    "        print(f\"ðŸŽ‰ New best model saved! Mean AUC: {best_mean_auc:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Training completed! Best Mean AUC: {best_mean_auc:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Analysis and Optimization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate improvement metrics\n",
    "if len(training_history['mean_auc']) >= 2:\n",
    "    stage1_best = max(training_history['mean_auc'][:EPOCHS//2])\n",
    "    stage2_best = max(training_history['mean_auc'][EPOCHS//2:])\n",
    "    improvement = stage2_best - stage1_best\n",
    "    \n",
    "    print(f\"Stage 1 Best AUC: {stage1_best:.4f}\")\n",
    "    print(f\"Stage 2 Best AUC: {stage2_best:.4f}\")\n",
    "    print(f\"Fine-tuning Improvement: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "if len(training_history['val_auc']) > 0:\n",
    "    best_aucs = training_history['val_auc'][np.argmax(training_history['mean_auc'])]\n",
    "    class_performance = [(LABELS[i], auc) for i, auc in enumerate(best_aucs) if not np.isnan(auc)]\n",
    "    class_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nBest Performing Classes:\")\n",
    "    for i, (label, auc) in enumerate(class_performance[:3]):\n",
    "        status = \"âœ…\" if auc >= 0.90 else \"ðŸŸ¡\" if auc >= 0.80 else \"ðŸ”´\"\n",
    "        print(f\"  {status} {label:25}: AUC = {auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nWorst Performing Classes (Need Attention):\")\n",
    "    for i, (label, auc) in enumerate(class_performance[-3:]):\n",
    "        status = \"âœ…\" if auc >= 0.90 else \"ðŸŸ¡\" if auc >= 0.80 else \"ðŸ”´\"\n",
    "        print(f\"  {status} {label:25}: AUC = {auc:.4f}\")\n",
    "\n",
    "# Recommendations for achieving 95%+ accuracy\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS FOR 95%+ ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if best_mean_auc < 0.95:\n",
    "    print(\"Current performance is below 95% target. Consider:\")\n",
    "    print(\"â€¢ Increase training epochs to 100-150\")\n",
    "    print(\"â€¢ Use test-time augmentation (TTA)\")\n",
    "    print(\"â€¢ Implement ensemble of multiple models\")\n",
    "    print(\"â€¢ Add more data augmentation techniques\")\n",
    "    print(\"â€¢ Use different loss functions (e.g., AUC loss)\")\n",
    "    print(\"â€¢ Try different learning rate schedules\")\n",
    "    print(\"â€¢ Consider using larger image sizes (384x384 or 512x512)\")\n",
    "else:\n",
    "    print(\"ðŸŽ‰ Congratulations! You've achieved 95%+ accuracy!\")\n",
    "    print(\"Consider these optimizations:\")\n",
    "    print(\"â€¢ Model compression for deployment\")\n",
    "    print(\"â€¢ Knowledge distillation\")\n",
    "    print(\"â€¢ Quantization for faster inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Time Augmentation (TTA) for Enhanced Performance\n",
    "def evaluate_with_tta(model, loader, num_augmentations=5):\n",
    "    \"\"\"Evaluate model with test-time augmentation for better performance\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    # Define TTA transforms\n",
    "    tta_transforms = [\n",
    "        A.Compose([\n",
    "            A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "            A.HorizontalFlip(p=flip_p),\n",
    "            A.Rotate(limit=rot_deg, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "            A.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                       std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "            ToTensorV2()\n",
    "        ]) for flip_p, rot_deg in [(0.0, 0), (1.0, 0), (0.0, 5), (0.0, -5), (0.5, 3)]\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating with TTA\"):\n",
    "            batch_predictions = []\n",
    "            \n",
    "            for tta_transform in tta_transforms:\n",
    "                tta_images = []\n",
    "                for img in images:\n",
    "                    # Convert tensor back to numpy for augmentation\n",
    "                    img_np = img.permute(1, 2, 0).numpy()\n",
    "                    img_np = (img_np * np.array([0.26862954, 0.26130258, 0.27577711]) + \n",
    "                             np.array([0.48145466, 0.4578275, 0.40821073])) * 255\n",
    "                    img_np = np.clip(img_np, 0, 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Apply TTA transform\n",
    "                    augmented = tta_transform(image=img_np)\n",
    "                    tta_images.append(augmented['image'])\n",
    "                \n",
    "                tta_batch = torch.stack(tta_images).to(DEVICE)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = model(tta_batch)\n",
    "                    batch_predictions.append(torch.sigmoid(outputs).cpu())\n",
    "            \n",
    "            # Average predictions across all augmentations\n",
    "            avg_predictions = torch.stack(batch_predictions).mean(dim=0)\n",
    "            all_outputs.append(avg_predictions.numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Compute AUC for each class\n",
    "    aucs = []\n",
    "    for i in range(NUM_CLASSES):\n",
    "        try:\n",
    "            if len(np.unique(all_labels[:, i])) > 1:\n",
    "                auc = roc_auc_score(all_labels[:, i], all_outputs[:, i])\n",
    "            else:\n",
    "                auc = np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing AUC for {LABELS[i]}: {e}\")\n",
    "            auc = np.nan\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    return aucs\n",
    "\n",
    "print(\"âœ… Test-Time Augmentation (TTA) function ready!\")\n",
    "print(\"Run the next cell to evaluate with TTA for potentially higher accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7bf581",
   "metadata": {},
   "source": [
    "## 8. Final Model Saving and Results Summary\n",
    "Save the final model and display comprehensive training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'training_history': training_history,\n",
    "    'final_mean_auc': training_history['mean_auc'][-1],\n",
    "    'best_mean_auc': best_mean_auc,\n",
    "    'config': {\n",
    "        'model_name': 'vit_giant_patch14_224',\n",
    "        'img_size': IMG_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'lr': LR,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'labels': LABELS\n",
    "    }\n",
    "}, 'chexpert_biomedclip_vit_final.pth')\n",
    "\n",
    "print('âœ… Final model saved as chexpert_biomedclip_vit_final.pth')\n",
    "print('âœ… Best model saved as chexpert_biomedclip_vit_best.pth')\n",
    "\n",
    "# Display final results summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: BiomedCLIP ViT-G/14 (Giant Vision Transformer)\")\n",
    "print(f\"Dataset: CheXpert\")\n",
    "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs Trained: {EPOCHS}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Best Mean AUC: {best_mean_auc:.4f}\")\n",
    "print(f\"Final Mean AUC: {training_history['mean_auc'][-1]:.4f}\")\n",
    "print(f\"Final Train Loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display best performing classes\n",
    "if len(training_history['val_auc']) > 0:\n",
    "    best_aucs = training_history['val_auc'][np.argmax(training_history['mean_auc'])]\n",
    "    valid_aucs = [(LABELS[i], auc) for i, auc in enumerate(best_aucs) if not np.isnan(auc)]\n",
    "    valid_aucs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nBest Model Performance by Class:\")\n",
    "    for label, auc in valid_aucs[:5]:  # Top 5\n",
    "        print(f\"  {label:25}: AUC = {auc:.4f}\")\n",
    "    \n",
    "    if len(valid_aucs) > 5:\n",
    "        print(\"  ...\")\n",
    "        for label, auc in valid_aucs[-3:]:  # Bottom 3\n",
    "            print(f\"  {label:25}: AUC = {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
